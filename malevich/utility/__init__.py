"""
THIS FILE IS AUTOGENERATED BY malevich PACKAGE INSTALLER.

THIS FILE CONTAINS VITAL INFORMATION ABOUT THE PACKAGE AND ITS CONTENTS.
DO NOT MODIFY THIS FILE MANUALLY.
"""

from malevich._autoflow.function import autotrace, sinktrace
from malevich._utility.registry import Registry
from malevich.models.nodes import OperationNode

from pydantic import BaseModel

class default_scheme(BaseModel):
	pass

class obj(BaseModel):
	path: str = None

class Links(BaseModel):
	link: str = None

class FilenameS3Key(BaseModel):
	filename: str = None
	s3key: str = None

class Filename(BaseModel):
	filename: str = None

Registry().register("23f7ce361db2c816a54eb53d14cf20764f7b85ea5727e3d418bfb06050c5990b", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "add_column",
})

@autotrace
def add_column(df, config: dict = {}):
    """Inserts a new column into a dataframe.

    Input:
        An arbitrary dataframe and context information
        with the following fields in the app config:
            - column: name of the new column
            - value: value of the new column
            - position: position of the new column

    Output:
        The input dataframe with the new column inserted at the specified position.

    Details:
        The function takes in a dataframe as an input and adds a new column
        at the specified position. The new column has a constant value provided
        by the user in the application configuration.

        If the position is negative, the new column will be inserted from the
        end of the dataframe. For example, a position of -1 will insert the
        new column as the last column in the dataframe.

    Configuration:
        - column (str) [optional, default is 'new_column']:
            The name of the new column.
        - value (any) [optional, default is 'new_value']:
            The value of the new column.
        - position (int) [optional, default is 0]:
            The position to insert the new column. If positive, the new column
            will be inserted from the beginning of the dataframe. If negative,
            the new column will be inserted from the end of the dataframe.

    Args:
        df: The input dataframe.
        context: The context information.

    Returns:
        The dataframe with new column.
    """
    return OperationNode(
        operation_id="23f7ce361db2c816a54eb53d14cf20764f7b85ea5727e3d418bfb06050c5990b",
        config=config,
        processor_id="add_column",
        package_id="utility",
    )

Registry().register("af3b4d180cbc342d9d8980524c7493f2d343f7714dbb48fb2280ae9cf194bbed", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "combine_vertical_id",
})

@autotrace
def combine_vertical_id(dataframe1, dataframe2, config: dict = {}):
    """Concatenates two dataframes vertically.

    Input:
        - Two Pandas dataframes of equal column length.

    Output:
        - The concatenated dataframe with adjusted column names.

    Details:
        The function takes in two equally sized
        dataframes and concatenates them vertically.

        The new column names are set according
        to the specified behavior in configuration.

        If `ignore_col_names` is set to True, column names are set generically,
        using `default_name_i` where `i` is the index of the column.
        If `ignore_col_names` is set to False, shared column names are preserved,
        while mismatched names are replaced by `default_name_i`.

    Note:
        The function raises a ValueError if the dataframes
        do not share the same number of columns.

    Configuration:
        - ignore_col_names (bool):
            determines whether to ignore current column names, defaults to False.
        - default_name (str):
            name template for generic columns names, defaults to 'col'.
        - ignore_index (bool):
            determines whether to ignore dataframe indexes during concatenation,
            defaults to False.

    Args:
        dataframe1, dataframe2 (pd.DataFrame): Dataframes to be merged.

    Returns:
        The concatenated dataframe (pd.DataFrame).
    """
    return OperationNode(
        operation_id="af3b4d180cbc342d9d8980524c7493f2d343f7714dbb48fb2280ae9cf194bbed",
        config=config,
        processor_id="combine_vertical_id",
        package_id="utility",
    )

Registry().register("9d3d55f775c31ea1bdd614905f32ff2e35873a6ddacfee1b93456831708c2131", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "download",
})

@autotrace
def download(links: Links, config: dict = {}):
    """Download files from the internet

    Inputs:
        A dataframe with a single column `link` containing links to files to download.

    Outputs:
        A dataframe with a single column `file` containing paths to downloaded files.

    Configuration:
        prefix (str, optional): a prefix to add to the paths of downloaded files.
            If not specified, files will be downloaded to the root of the app directory.

    Args:
        links (DF[Links]): Dataframe with links to download
        context (Context): Context object

    Returns:
        Dataframe with paths to downloaded files
    """
    return OperationNode(
        operation_id="9d3d55f775c31ea1bdd614905f32ff2e35873a6ddacfee1b93456831708c2131",
        config=config,
        processor_id="download",
        package_id="utility",
    )

Registry().register("60844c2c5f972321f0c10039b46843741be4daac55c7c4ecfeaabf5b9b35f604", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "get_links_to_files",
})

@autotrace
def get_links_to_files(df, config: dict = {}):
    """Get links to files produced during the workflow execution.

    Input:
        An arbitrary dataframe.

    Output:
        The same dataframe, but with all file paths replaced
        with openable links to the files.

    Configuration:
        - expiration (int): The number of seconds after which the link
            will expire. Defaults to 6 hours. Maximum is 24 hours.

    Args:
        df (DF):
            An arbitrary dataframe.
        ctx (Context):
            The context object.

    Returns:
        DF:
            The same dataframe, but with all file paths replaced
            with openable links to the files.
    """
    return OperationNode(
        operation_id="60844c2c5f972321f0c10039b46843741be4daac55c7c4ecfeaabf5b9b35f604",
        config=config,
        processor_id="get_links_to_files",
        package_id="utility",
    )

Registry().register("cb2d2ad53c1a87398e4ce30ee705cc8be167edcb94949f5bf2441f3e80b86196", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "pattern_match_processor",
})

@autotrace
def pattern_match_processor(dataframe, config: dict = {}):
    """
    This processor finds all the fragments
    that match a certain pattern within each cell.

    They are later joined with a certain symbol and saved in an analogue cell

    Args:
        dataframe: the input
        context: the usual Malevich context, we expect the field 'pattern' from it.

    Returns: a new dataframe with the same dimensions and names as the input dataframe.
    """
    return OperationNode(
        operation_id="cb2d2ad53c1a87398e4ce30ee705cc8be167edcb94949f5bf2441f3e80b86196",
        config=config,
        processor_id="pattern_match_processor",
        package_id="utility",
    )

Registry().register("9bec55d3ab92a55f1f3087c1c55b15c4e1db0a3502ce68b59db38dee48b7f881", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "merge",
})

@sinktrace
def merge(*args, config: dict = {}):
    """Merges multiple dataframes into one

    Input:
        dfs: An iterable containing multiple datafr ames to be merged.

    Configuration:
        - how (str) [optional, default is 'inner']:
            The type of merge to be performed.

            Possible values:
                - 'inner':
                    Use intersection of keys from both frames,
                    similar to a SQL inner join;
                - 'outer':
                    Use union of keys from both frames,
                    similar to a SQL full outer join;
                - 'left':
                    Use only keys from left frame,
                    similar to a SQL left outer join;
                - 'right':
                    Use only keys from right frame,
                    similar to a SQL right outer join;
                - 'cross':
                    Create a cartesian product from both frames,
                    similar to a SQL cross join.
        - both_on (str or tuple) [optional]:
            Column name or 'index' to merge on.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all dataframes
        - left_on (str or list) [optional]:
            Column name or 'index' to join on in the left DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all but last dataframes
        - right_on (str or list) [optional]:
            Column name or 'index' to join on in the right DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all but first dataframes
        - suffixes (tuple) [optional, default is ('_0', '_1')]:
            Suffix to apply to overlapping column names in the left and right dataframes

    Notes:
        If both 'both_on' and 'left_on/right_on' are provided,
        'both_on' will be ignored.

        Dataframes are merged iteratively from left to right.

        If using left_on column, all dataframes except
        the last one should have the column.

        If using right_on column, all dataframes except
        the first one should have the column.

    Args:
        dfs: DFS containing DataFrames to be merged.

    Returns:
        The merged dataframe
    """
    return OperationNode(
        operation_id="9bec55d3ab92a55f1f3087c1c55b15c4e1db0a3502ce68b59db38dee48b7f881",
        config=config,
        processor_id="merge",
        package_id="utility",
    )

Registry().register("8584d9e3d2fe56391019663212234e4421d1f3b524946f25471a16ceba5decea", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "merge_two",
})

@autotrace
def merge_two(df_1, df_2, config: dict = {}):
    """Merges multiple dataframes into one

    Input:
        Two dataframes to be merged.

    Configuration:
        - how (str) [optional, default is 'inner']:
            The type of merge to be performed.

            Possible values:
                - 'inner':
                    Use intersection of keys from both frames,
                    similar to a SQL inner join;
                - 'outer':
                    Use union of keys from both frames,
                    similar to a SQL full outer join;
                - 'left':
                    Use only keys from left frame,
                    similar to a SQL left outer join;
                - 'right':
                    Use only keys from right frame,
                    similar to a SQL right outer join;
                - 'cross':
                    Create a cartesian product from both frames,
                    similar to a SQL cross join.
        - both_on (str or tuple) [optional]:
            Column name or 'index' to merge on.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all dataframes
        - left_on (str or list) [optional]:
            Column name or 'index' to join on in the left DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in the left dataframe
        - right_on (str or list) [optional]:
            Column name or 'index' to join on in the right DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in the right dataframe
        - suffixes (tuple) [optional, default is ('_0', '_1')]:
            Suffix to apply to overlapping column names in the left and right dataframes

    Notes:
        If both 'both_on' and 'left_on/right_on' are provided,
        'both_on' will be ignored.

        Dataframes are merged iteratively from left to right.

        If using left_on column, all dataframes except
        the last one should have the column.

        If using right_on column, all dataframes except
        the first one should have the column.

    Args:
        df_1: Left dataframe
        df_2: Right dataframe

    Returns:
        The merged dataframe
    """
    return OperationNode(
        operation_id="8584d9e3d2fe56391019663212234e4421d1f3b524946f25471a16ceba5decea",
        config=config,
        processor_id="merge_two",
        package_id="utility",
    )

Registry().register("c80cced2444ecdc7765205dc54c543cfd1fb44602ed40de1ff86db3ec0842e50", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "merge_three",
})

@autotrace
def merge_three(df_1, df_2, df_3, config: dict = {}):
    """Merges multiple dataframes into one

    Input:
        Three dataframes to be merged.

    Configuration:
        - how (str) [optional, default is 'inner']:
            The type of merge to be performed.

            Possible values:
                - 'inner':
                    Use intersection of keys from both frames,
                    similar to a SQL inner join;
                - 'outer':
                    Use union of keys from both frames,
                    similar to a SQL full outer join;
                - 'left':
                    Use only keys from left frame,
                    similar to a SQL left outer join;
                - 'right':
                    Use only keys from right frame,
                    similar to a SQL right outer join;
                - 'cross':
                    Create a cartesian product from both frames,
                    similar to a SQL cross join.
        - both_on (str or tuple) [optional]:
            Column name or 'index' to merge on.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all dataframes
        - left_on (str or list) [optional]:
            Column name or 'index' to join on in the left DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all but last dataframes
        - right_on (str or list) [optional]:
            Column name or 'index' to join on in the right DataFrame.
            If 'index', the index of the dataframe will be used.
            If column name, the column should be present in all but first dataframes
        - suffixes (tuple) [optional, default is ('_0', '_1')]:
            Suffix to apply to overlapping column names in the left and right dataframes

    Notes:
        If both 'both_on' and 'left_on/right_on' are provided,
        'both_on' will be ignored.

        Dataframes are merged iteratively from left to right.

        If using left_on column, all dataframes except
        the last one should have the column.

        If using right_on column, all dataframes except
        the first one should have the column.

    Args:
        df_1: Left dataframe
        df_2: Right dataframe

    Returns:
        The merged dataframe
    """
    return OperationNode(
        operation_id="c80cced2444ecdc7765205dc54c543cfd1fb44602ed40de1ff86db3ec0842e50",
        config=config,
        processor_id="merge_three",
        package_id="utility",
    )

Registry().register("e5f03fc0151a383417464b53399fc30d0891bf131714c4f1e9d46e4a69b8ea66", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "pass_through",
})

@autotrace
def pass_through(df, config: dict = {}):
    """None"""
    return OperationNode(
        operation_id="e5f03fc0151a383417464b53399fc30d0891bf131714c4f1e9d46e4a69b8ea66",
        config=config,
        processor_id="pass_through",
        package_id="utility",
    )

Registry().register("5a6e055085c4174c42df018c753c49a04d9934e05fe4da93d2865715880d8672", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "rename",
})

@autotrace
def rename(df, config: dict = {}):
    """Renames columns in a dataframe.

    Input:
        - DataFrame with the columns to be renamed.

    Configuration:
        - Provides mapping of old column names to their new names.

        For example, if the dataframe has columns 'a', 'b', 'c' and we want to rename
        'a' to 'A', 'b' to 'B', and 'c' to 'C', the configuration should be:

        {
            'a': 'A',
            'b': 'B',
            'c': 'C'
        }

    Details:
        This processor renames columns in the dataframe based on provided mappings.
        User needs to provide a dictionary in the configuration hat specifies old
        column names as keys and new column names as values.

    Args:
        df: DataFrame in which to rename columns.

    Returns:
        DataFrame with renamed columns.
    """
    return OperationNode(
        operation_id="5a6e055085c4174c42df018c753c49a04d9934e05fe4da93d2865715880d8672",
        config=config,
        processor_id="rename",
        package_id="utility",
    )

Registry().register("6a0a049a6b3a7ee33527f86195bf4ee89ebc8caeb96289b4f11ed9573f9ea7ae", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "s3_save",
})

@autotrace
def s3_save(dfs, config: dict = {}):
    """Saves dataframes to S3.

    Input:
        dfs: Multiple dataframes to be saved.

    Configuration:
        - names (list of str or str) [optional]:
            Names of the dataframes to be saved. If a list is provided, it should
            have the same length as the number of dataframes. If a string is provided,
            it is used as a format string to generate the names of the dataframes.
            Available format variables:
                - {ID}: index of the dataframe
            If the number of dataframes is greater than the length of the list or
            the number of format variables in the string, default names are used
            for the remaining dataframes.
        - append_run_id (bool) [optional, default is False]:
            If True, the run_id is appended to the names of the dataframes.
        - extra_str (str) [optional]:
            If provided, it is appended to the names of the dataframes.

        Also, the app should be provided with parameters to connect to S3:
            - aws_access_key_id (str):
                AWS access key ID.
            - aws_secret_access_key (str):
                AWS secret access key.
            - bucket_name (str):
                Name of the S3 bucket.
            - endpoint_url (str) [optional]:
                Endpoint URL of the S3 bucket.
            - aws_region (str) [optional]:
                AWS region of the S3 bucket.

    Details:
        This processor saves dataframes to S3. User can provide names for the
        dataframes to be saved. If no names are provided, default names are used.
        If the number of dataframes is greater than the length of the list of names
        or the number of format variables in the string, default names are used
        for the remaining dataframes.

        If `append_run_id` is True, the run_id is appended to the names of the
        dataframes. If `extra_str` is provided, it is appended to the names of
        the dataframes.

        The dataframes are saved to S3 using the following key:
            <EXTRA_STR>/<RUN_ID>/<NAME>

        A common use case of extra_str is to save dataframes to different folders
        within the S3 bucket. For example, if extra_str is 'train', the dataframes
        are saved to the following key:

            train/<RUN_ID>/<NAME>

    Output:
        The same as the input.

    Args:
        dfs: DFS containing DataFrames to be saved.

    Returns:
        The same dataframes as the input.

    """
    return OperationNode(
        operation_id="6a0a049a6b3a7ee33527f86195bf4ee89ebc8caeb96289b4f11ed9573f9ea7ae",
        config=config,
        processor_id="s3_save",
        package_id="utility",
    )

Registry().register("10c9aa54f575d063a6381c12f0335af25b907453071938f0e7c169c1d4baebda", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "s3_save_files_auto",
})

@autotrace
def s3_save_files_auto(files: Filename, config: dict = {}):
    """Saves files from local file system to S3 preserving the original names.

    Input:
         files:
            A dataframe with one column:
                - 'filename' (str):
                    Contains the names of the files to be saved.

    Configuration:
        - append_run_id (bool) [optional, default is False]:
            If True, the run_id is appended to the names of the files.
        - extra_str (str) [optional]:
            If provided, it is appended to the names of the files.

        Also, the app should be provided with parameters to connect to S3:
            - aws_access_key_id (str):
                AWS access key ID.
            - aws_secret_access_key (str):
                AWS secret access key.
            - bucket_name (str):
                Name of the S3 bucket.
            - endpoint_url (str) [optional]:
                Endpoint URL of the S3 bucket.
            - aws_region (str) [optional]:
                AWS region of the S3 bucket.

    Details:
        Files are expected to be in the share folder e.g. should be shared with
        `context.share(<FILE>)` before by a previous processor.

        The files are saved to S3 using the following key:
            <EXTRA_STR>/<RUN_ID>/<SHARED_FILE_NAME>

        A common use case of extra_str is to save files to different folders
        within the S3 bucket. For example, if extra_str is 'train', the files
        are saved to the following key:

                train/<RUN_ID>/<SHARED_FILE_NAME>

    Output:
        The same as the input.

    Args:
        files: DF containing filenames to be saved.

    Returns:
        The same dataframe as the input.
    """
    return OperationNode(
        operation_id="10c9aa54f575d063a6381c12f0335af25b907453071938f0e7c169c1d4baebda",
        config=config,
        processor_id="s3_save_files_auto",
        package_id="utility",
    )

Registry().register("cc577b0769b1735bbcd4d91cfddfd6dc3bf5aa079c195759554458ff69b332d5", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "s3_save_files",
})

@autotrace
def s3_save_files(files: FilenameS3Key, config: dict = {}):
    """Saves files from local file system to S3.

    Input:
        files:
            A dataframe with two columns: 'filename' and 's3key'. 'filename' contains
            the names of the files to be saved. 's3key' contains the S3 key for each
            file.

    Configuration:
        - append_run_id (bool) [optional, default is False]:
            If True, the run_id is appended to the names of the files.

        Also, the app should be provided with parameters to connect to S3:
            - aws_access_key_id (str):
                AWS access key ID.
            - aws_secret_access_key (str):
                AWS secret access key.
            - bucket_name (str):
                Name of the S3 bucket.
            - endpoint_url (str) [optional]:
                Endpoint URL of the S3 bucket.
            - aws_region (str) [optional]:
                AWS region of the S3 bucket.

    Details:
        Files are expected to be in the share folder e.g. should be shared with
        `context.share(<FILE>)` before by a previous processor.

        The files are saved to S3 using the following key:
            <EXTRA_STR>/<RUN_ID>/<S3_KEY>

        A common use case of extra_str is to save files to different folders
        within the S3 bucket. For example, if extra_str is 'train', the files
        are saved to the following key:

                train/<RUN_ID>/<S3_KEY>

        S3 keys might contain following variables:
            - {ID}: index of the dataframe
            - {FILE}: base filename, for example 'file.csv' for 'path/to/file.csv'
            - {RUN_ID}: run_id

        For example, if the S3 key is 'train/{RUN_ID}/{FILE}', file name is 'file.csv',
        run_id is 'run_1', the file will be saved to 'train/run_1/file.csv'.

    Output:
        The same as the input.

    Args:
        files: DF containing filenames to be saved.

    Returns:
        The same dataframe as the input.
    """
    return OperationNode(
        operation_id="cc577b0769b1735bbcd4d91cfddfd6dc3bf5aa079c195759554458ff69b332d5",
        config=config,
        processor_id="s3_save_files",
        package_id="utility",
    )

Registry().register("649f766f3043b8841363d3315ef649dd62ed39b1f40a5e121b1713c05e669c2c", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "s3_download_files",
})

@autotrace
def s3_download_files(files: FilenameS3Key, config: dict = {}):
    """Downloads files from S3 to local file system.

    Input:
        files:
            A dataframe with two columns: 'filename' and 's3key'. 'filename' contains
            the names of the files to be downloaded. 's3key'
            contains the S3 key for each file.

    Configuration:

        The app's only configuration is the connection to S3:
            - aws_access_key_id (str):
                AWS access key ID.
            - aws_secret_access_key (str):
                AWS secret access key.
            - bucket_name (str):
                Name of the S3 bucket.
            - endpoint_url (str) [optional]:
                Endpoint URL of the S3 bucket.
            - aws_region (str) [optional]:
                AWS region of the S3 bucket.

    Details:
       Files are downloaded by their S3 keys. The files are shared across processors
       under keys specified by `filename` column.

       For example, for the dataframe:
            | filename  |            s3key          |
            | --------  |            -----          |
            | file1.csv | path/to/some_file.csv     |

            The file is assumed to be in S3 under the key `path/to/some_file.csv`.
            The file is downloaded from S3 and shared under the key `file1.csv`.

    Output:
        The same as the input.

    Args:
        files: DF containing filenames to be downloaded.

    Returns:
        The same dataframe as the input.
    """
    return OperationNode(
        operation_id="649f766f3043b8841363d3315ef649dd62ed39b1f40a5e121b1713c05e669c2c",
        config=config,
        processor_id="s3_download_files",
        package_id="utility",
    )

Registry().register("27fb437dd6275c6da7e2ad5c1b1dbb275885682f6e91ff5711c2c37874bbd56a", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "filter",
})

@autotrace
def filter(df, config: dict = {}):
    """Filters rows by a number of conditions

    Inputs:
        An arbitrary dataframe to filter rows from

    Outputs:
        A filtered dataframe

    A single condition is a dictionary with the following keys:
    - column: the column to filter on
    - operation: the operation to perform
    - value: the value to filter on
    - type: the type of the value to filter on (optional)

    Example:
    {
        "conditions": [
            {
                "column": "age",
                "operation": "greater",
                "value": 18,
                "type": "int"
            },
            {
                "column": "name",
                "operation": "like",
                "value": "John"
            }
        ]
    }

    Supported operations:
    - equal
    - not_equal
    - greater
    - greater_equal
    - less
    - less_equal
    - in
    - not_in
    - like
    - not_like
    - is_null
    - is_not_null

    Supported types:
    - int
    - float
    - bool
    - str

    Args:
        df: the dataframe to filter rows from
        context: the context of the current request

    Returns:
        A filtered dataframe
    """
    return OperationNode(
        operation_id="27fb437dd6275c6da7e2ad5c1b1dbb275885682f6e91ff5711c2c37874bbd56a",
        config=config,
        processor_id="filter",
        package_id="utility",
    )

Registry().register("f55aa034e097b62323c9b84c78c06ac1ce71b4da59b2bc84e0f67924e485967f", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "locs",
})

@autotrace
def locs(df, config: dict = {}):
    """ Locate Statically - Extracts a subset of the dataframe

    Input:
        A DataFrame to be processed.

    Configuration:
        The app configuration should contain at least one of the following fields:

        - column (str): The column to be extracted.
        - columns (list[str]): The columns to be extracted.
        - column_idx (int): The column index to be extracted.
        - column_idxs (list[int]): The column indexes to be extracted.
        - row (int): The row to be extracted.
        - rows (list[int]): The rows to be extracted.
        - row_idx (int): The row index to be extracted.
        - row_idxs (list[int]): The row indexes to be extracted.

        Multiple fields may be provided and in such case,
        the function will extract the intersection of the fields.

    Notes:
        At least one of the above fields should be provided for the function to work.

        Moreover, the dataframe is processed in column first then row order.
        Queries are executed from the most specific to the least within each category.
        If both specific and general conditions are given, the function prioritizes
        the specific ones to maintain consistency.

    Args:
        df (pd.DataFrame): The DataFrame to be processed.

    Returns:
        The extracted subset from the DataFrame.
    """
    return OperationNode(
        operation_id="f55aa034e097b62323c9b84c78c06ac1ce71b4da59b2bc84e0f67924e485967f",
        config=config,
        processor_id="locs",
        package_id="utility",
    )

Registry().register("40c3688cb43a48ef9f7dce3048d82dfbfb6344d83f1b1975deedbf6ed0732cff", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "subset",
})

@autotrace
def subset(dfs, config: dict = {}):
    """Select a subset of dataframes from the list of dataframes.

    Input:
        A number of dataframes.

    Output:
        A subset of dataframes or a single dataframe.

    Configuration:
        expr: str
            A comma-separated list of integers or slices, e.g. `0,1:3,5:7,6,9:10`.
            The first dataframe has index 0.

    Details:
        The `expr` field should be a comma-separated list of integers or slices,
        e.g. `0,1:3,5:7,6,9:10`.

        Zero-based indexing is used for the dataframes.

        `expr` is matched against the regular expression
            `^(\d+|(\d+\:\d+))(\,(\d+|(\d+\:\d+)))*$`.

        If the expression contains only one element, a single dataframe is
        returned. Otherwise, a slice of dataframes is returned.

    Args:
        dfs: A number of arbitrary dataframes.

    Returns:
        A subset of dataframes or a single dataframe if the subset contains
        a single index.
    """
    return OperationNode(
        operation_id="40c3688cb43a48ef9f7dce3048d82dfbfb6344d83f1b1975deedbf6ed0732cff",
        config=config,
        processor_id="subset",
        package_id="utility",
    )

Registry().register("536ebcde1978a849f648e43bec456663e69e8e19f1c5cd44ec8039e9f11a8f5e", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "squash",
})

@autotrace
def squash(df, config: dict = {}):
    """Squash multiple rows into one row.

    Inputs:
        An arbitrary dataframe with columns that contain multiple values.

    Outputs:
        A dataframe with the same columns as the input dataframe, but with
        multiple rows for each input row.

    Configuration:
        by (str): The column to group by. If not specified, all
            columns will be squashed.

        delim (str): The delimiter used to separate values in the columns. If
            not specified, the default delimiter is a comma (,).

    Args:
        df (DF[Any]): Dataframe
        context (Context): Context object

    Returns:
        Dataframe with squashed rows
    """
    return OperationNode(
        operation_id="536ebcde1978a849f648e43bec456663e69e8e19f1c5cd44ec8039e9f11a8f5e",
        config=config,
        processor_id="squash",
        package_id="utility",
    )

Registry().register("a72667e3d8a720fa14cc9556f7753d622849a3a1cf59038abd2c013b1e4d1472", {
    "image_ref": ('dependencies', 'utility', 'options', 'image_ref'),
    "image_auth_user": ('dependencies', 'utility', 'options', 'image_auth_user'),
    "image_auth_pass": ('dependencies', 'utility', 'options', 'image_auth_pass'),
    "processor_id": "unwrap",
})

@autotrace
def unwrap(df, config: dict = {}):
    """Unwrap columns with multiple values into multiple rows.

    If a column contains multiple values, this processor will create a new row
    for each value. The new rows will be identical to the original row except
    for the column that was unwrapped.

    For example, if the input dataframe is:

    | id | name | tags |
    |----|------|------|
    | 1  | A    | a,b  |
    | 2  | B    | c    |

    Then the output dataframe will be:

    | id | name | tags |
    |----|------|------|
    | 1  | A    | a    |
    | 1  | A    | b    |
    | 2  | B    | c    |


    Inputs:

        An arbitrary dataframe with columns that contain multiple values.

    Outputs:

        A dataframe with the same columns as the input dataframe, but with
        multiple rows for each input row.

    Configuration:

        columns (list of str): The columns to unwrap. If not specified, all
            columns will be unwrapped.

        delimiter (str): The delimiter used to separate values in the columns. If
            not specified, the default delimiter is a comma (,).

    Notes:

        Be careful when using this processor with columns that contain
        non-text values. For example, if a column contains a list of numbers,
        and the delimiter is a dot, then the processor will treat each number
        as a separate value. For example, if the input dataframe is:

    | id | name | numbers |
    |----|------|---------|
    | 1  | A.B  | 1.2     |

        Then the output dataframe will be:

    | id | name | numbers |
    |----|------|---------|
    | 1  | A    | 1       |
    | 1  | A    | 2       |
    | 1  | B    | 1       |
    | 1  | B    | 2       |


    Args:

        df (pandas.DataFrame): The input dataframe.

        config (dict): The configuration for this processor.

    Returns:

        The same dataframe as the input dataframe, but with multiple rows for
        each input row.
    """
    return OperationNode(
        operation_id="a72667e3d8a720fa14cc9556f7753d622849a3a1cf59038abd2c013b1e4d1472",
        config=config,
        processor_id="unwrap",
        package_id="utility",
    )

# match_pattern_input = ...


# merge_2_from_extra = ...


# download_from_collection = ...


# sink2_from_extra = ...


# connect_to_s3 = ...


# save_output = ...


__checksum = 'b996cba66c39e810124e34372637a5a9c0f5c4510490254862e0dbf72a6e0a30'
